{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqM-T1RTzY6C"
      },
      "source": [
        "Para executar isso, pressione \"*Runtime*\" e pressione \"*Run all*\" em uma inst√¢ncia **gr√°tis** do Tesla T4 Google Colab!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://ollama.com/\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/ollama.png\" height=\"44\"></a>\n",
        "<a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Entre no Discord se precisar de ajuda + ‚≠ê <i>Marque-nos com uma estrela no <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>\n",
        "\n",
        "Para instalar o Unsloth no seu pr√≥prio computador, siga as instru√ß√µes de instala√ß√£o na nossa p√°gina do Github [aqui](https://github.com/unslothai/unsloth#installation-instructions---conda).\n",
        "\n",
        "Voc√™ aprender√° como fazer [prepara√ß√£o de dados](#Dados) e importar um CSV, como [treinar](#Treinar), como [executar o modelo](#Infer√™ncia) e [como exportar para o Ollama!](#Ollama)\n",
        "\n",
        "[Unsloth](https://github.com/unslothai/unsloth) agora permite que voc√™ ajuste automaticamente e crie um [Modelfile](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) e exporte para [Ollama](https://ollama.com/)! Isso torna o ajuste fino muito mais f√°cil e fornece um fluxo de trabalho perfeito de `Unsloth` para `Ollama`!\n",
        "\n",
        "**[NOVO]** Agora permitimos o upload de arquivos CSV e Excel - experimente [aqui](https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing) usando o conjunto de dados do Titanic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2v_X2fA0Df5"
      },
      "source": [
        "* Oferecemos suporte a Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen, TinyLlama, Vicuna, Open Hermes etc\n",
        "* Oferecemos suporte a LoRA de 16 bits ou QLoRA de 4 bits. Ambos 2x mais r√°pidos.\n",
        "* `max_seq_length` pode ser definido como qualquer coisa, j√° que fazemos o dimensionamento autom√°tico de RoPE por meio do m√©todo [kaiokendev's](https://kaiokendev.github.io/til).\n",
        "* Com [PR 26037](https://github.com/huggingface/transformers/pull/26037), oferecemos suporte ao download de modelos de 4 bits **4x mais r√°pido**! [Nosso reposit√≥rio](https://huggingface.co/unsloth) tem modelos Llama e Mistral de 4 bits.\n",
        "* [**NOVO**] Tornamos o Phi-3 M√©dio / Mini **2x mais r√°pido**! Veja nosso [caderno Phi-3 Medium](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "f505fcc71858474e8474201e3aae5d37",
            "22e1dd61829149eb9014e60a3fbb315e",
            "05005f157d65410ea831c6e7e48ecf9c",
            "240fb241a53c4e96b54b655b0e5ec7c6",
            "2c4b5d605822410d9c79d716106acd39",
            "35cc6f434cb74f988fdd3d16a89ca46e",
            "57c0f79c22a044d59a861f305c6e2c68",
            "e68b1673cfea443b8fd090f2cd96606b",
            "f3d8d5c37e834fe0a6717db68bbf9791",
            "65ca2f08912b4111aa8ec4639e6e0fb4",
            "db37443e50554e23b911f1626e0c2381"
          ]
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "1336f5f5-4977-4099-e330-ace06de4a277"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "==((====))==  Unsloth 2024.9: Fast Llama patching. Transformers = 4.44.2.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.4.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f505fcc71858474e8474201e3aae5d37"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 8192 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
        "    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "Agora adicionamos adaptadores LoRA, ent√£o precisamos atualizar apenas 1 a 10% de todos os par√¢metros!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = True,  # N√≥s apoiamos LoRA estabilizado por classifica√ß√£o\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Prepara√ß√£o de dados\n",
        "Agora usamos o conjunto de dados Alpaca de [vicgalle](https://huggingface.co/datasets/vicgalle/alpaca-gpt4), que √© uma vers√£o de 52K do [conjunto de dados Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) original gerado do GPT4. Voc√™ pode substituir esta se√ß√£o de c√≥digo pela sua pr√≥pria prepara√ß√£o de dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvOPfPnet76H"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"vicgalle/alpaca-gpt4\", split = \"train\")\n",
        "print(dataset.column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg4_dG-m0Cz4"
      },
      "source": [
        "Um problema √© que esse conjunto de dados tem v√°rias colunas. Para que `Ollama` e `llama.cpp` funcionem como um Chatbot `ChatGPT` personalizado, precisamos ter apenas 2 colunas - uma coluna `instruction` e uma coluna `output`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTQR4jrDMcJf"
      },
      "outputs": [],
      "source": [
        "print(dataset.column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwEbRFl0Mf3E"
      },
      "source": [
        "Para resolver isso, faremos o seguinte:\n",
        "* Mesclar todas as colunas em 1 prompt de instru√ß√£o.\n",
        "* Lembre-se de que os LLMs s√£o preditores de texto, ent√£o podemos personalizar a instru√ß√£o para o que quisermos!\n",
        "* Use a fun√ß√£o `to_sharegpt` para fazer esse processo de mesclagem de colunas!\n",
        "\n",
        "Por exemplo, abaixo, em nosso [notebook de ajuste fino do Titanic CSV](https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing), mesclamos v√°rias colunas em 1 prompt:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/nightly/images/Merge.png\" height=\"100\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w61VJ7rQM8jT"
      },
      "source": [
        "Para mesclar v√°rias colunas em 1, use `merged_prompt`.\n",
        "* Coloque todas as colunas entre chaves `{}`.\n",
        "* O texto opcional deve ser inclu√≠do em `[[]]`. Por exemplo, se a coluna \"Pclass\" estiver vazia, a fun√ß√£o de mesclagem n√£o mostrar√° o texto e ignorar√° isso. Isso √© √∫til para conjuntos de dados com valores ausentes.\n",
        "* Voc√™ pode selecionar todas as colunas ou algumas!\n",
        "* Selecione a sa√≠da ou a coluna de destino/previs√£o em `output_column_name`. Para o conjunto de dados Alpaca, ser√° `output`.\n",
        "\n",
        "Para fazer o ajuste fino lidar com v√°rias voltas (como no ChatGPT), temos que criar um conjunto de dados \"falso\" com v√°rias voltas - usamos `conversation_extension` para selecionar aleatoriamente algumas conversas do conjunto de dados e agrup√°-las em 1 conversa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZxeGSeX0CR8"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "import json\n",
        "from unsloth import to_sharegpt\n",
        "\n",
        "# Carregar o dataset \"vicgalle/alpaca-gpt4\" e descartar o campo \"text\"\n",
        "dataset1 = load_dataset(\"vicgalle/alpaca-gpt4\", split=\"train\").remove_columns([\"text\"])\n",
        "\n",
        "# Carregar o JSON com os campos ['instruction', 'input', 'output']\n",
        "with open(\"lostmine_dataset.json\") as f:\n",
        "    data_json = json.load(f)\n",
        "\n",
        "data_dict = {\n",
        "    \"instruction\": [example[\"instruction\"] for example in data_json],\n",
        "    \"input\": [example[\"input\"] for example in data_json],\n",
        "    \"output\": [example[\"output\"] for example in data_json]\n",
        "}\n",
        "# Converter o JSON para um Dataset Hugging Face\n",
        "dataset2 = Dataset.from_dict(data_dict)\n",
        "print(dataset1.column_names)\n",
        "print(dataset2.column_names)\n",
        "# Concatenar os datasets\n",
        "final_dataset = Dataset.from_dict({\n",
        "    \"instruction\": dataset1[\"instruction\"] + dataset2[\"instruction\"],\n",
        "    \"input\": dataset1[\"input\"] + dataset2[\"input\"],\n",
        "    \"output\": dataset1[\"output\"] + dataset2[\"output\"]\n",
        "})\n",
        "# Concatenar os campos 'instruction' e 'input' no dataset1\n",
        "dataset = to_sharegpt(\n",
        "    final_dataset,\n",
        "    merged_prompt=\"{instruction}[[\\nYour input is:\\n{input}]]\",\n",
        "    output_column_name=\"output\",\n",
        "    conversation_extension=5,\n",
        ")\n",
        "# Verificar os campos finais\n",
        "print(dataset.column_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Kh90vpD1jYJ"
      },
      "source": [
        "Por fim, use `standardize_sharegpt` para corrigir o conjunto de dados!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPwDXBvP1g8S"
      },
      "outputs": [],
      "source": [
        "from unsloth import standardize_sharegpt\n",
        "dataset = standardize_sharegpt(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GThrcKACxTe2"
      },
      "source": [
        "### Modelos de bate-papo personaliz√°veis\n",
        "\n",
        "Voc√™ tamb√©m precisa especificar um modelo de bate-papo. Anteriormente, voc√™ podia usar o formato Alpaca, conforme mostrado abaixo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVBanRIJRAcQ"
      },
      "outputs": [],
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTzZ5oZrxkFz"
      },
      "source": [
        "Agora, voc√™ tem que usar `{INPUT}` para a instru√ß√£o e `{OUTPUT}` para a resposta.\n",
        "\n",
        "Tamb√©m permitimos que voc√™ use um campo opcional `{SYSTEM}`. Isso √© √∫til para o Ollama quando voc√™ quer usar um prompt de sistema personalizado (tamb√©m como no ChatGPT).\n",
        "\n",
        "Voc√™ tamb√©m n√£o pode colocar um campo `{SYSTEM}` e apenas colocar texto simples.\n",
        "\n",
        "```python\n",
        "chat_template = \"\"\"{SYSTEM}\n",
        "USU√ÅRIO: {INPUT}\n",
        "ASSISTENTE: {OUTPUT}\"\"\"\n",
        "```\n",
        "\n",
        "Use abaixo se quiser usar o formato de prompt Llama-3. Voc√™ deve usar o modelo `instruct` e n√£o o `base` se usar isso!\n",
        "```python\n",
        "chat_template = \"\"\"<|begin_of_text|><|start_header_id|>sistema<|end_header_id|>\n",
        "\n",
        "{SISTEMA}<|eot_id|><|start_header_id|>usu√°rio<|end_header_id|>\n",
        "\n",
        "{ENTRADA}<|eot_id|><|start_header_id|>assistente<|end_header_id|>\n",
        "\n",
        "{SA√çDA}<|eot_id|>\"\"\"\n",
        "```\n",
        "\n",
        "Para o formato ChatML:\n",
        "```python\n",
        "chat_template = \"\"\"<|im_start|>sistema\n",
        "{SISTEMA}<|im_end|>\n",
        "<|im_start|>usu√°rio\n",
        "{ENTRADA}<|im_end|>\n",
        "<|im_start|>assistente\n",
        "{SA√çDA}<|im_end|>\"\"\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK-_ncj-RCNy"
      },
      "source": [
        "O problema √© que o formato Alpaca tem 3 campos, enquanto os chatbots estilo OpenAI devem usar apenas 2 campos (instru√ß√£o e resposta). √â por isso que usamos a fun√ß√£o `to_sharegpt` para mesclar essas colunas em 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOGaZf1sdLlr"
      },
      "outputs": [],
      "source": [
        "chat_template = \"\"\"Below are some instructions that describe some tasks. Write responses that appropriately complete each request.\n",
        "\n",
        "### Instruction:\n",
        "{INPUT}\n",
        "\n",
        "### Response:\n",
        "{OUTPUT}\"\"\"\n",
        "\n",
        "from unsloth import apply_chat_template\n",
        "dataset = apply_chat_template(\n",
        "    dataset,\n",
        "    tokenizer = tokenizer,\n",
        "    chat_template = chat_template,\n",
        "    default_system_message = \"Voc√™ √© uma Intelig√™ncia Artificial atuando como Mestre de RPG de mesa, semelhante ao Dungeons & Dragons. Sua tarefa √© narrar hist√≥rias, conduzir aventuras e interagir com os jogadores de acordo com as regras do jogo. Aqui est√£o suas diretrizes:\\n\\nReceber Fichas de Personagem: Quando um jogador fornecer a ficha do seu personagem, voc√™ deve analisar os atributos, habilidades, equipamentos e hist√≥rico do personagem.\\n\\nSelecionar e Narrar Hist√≥rias: Com base na hist√≥ria selecionada, voc√™ iniciar√° a narrativa, descrevendo o ambiente, NPCs (Personagens N√£o Jog√°veis) e eventos. Use descri√ß√µes v√≠vidas para imergir os jogadores na aventura.\\n\\nIntera√ß√£o e Decis√µes dos Jogadores: Durante a narrativa, voc√™ dever√° propor situa√ß√µes que exijam decis√µes dos jogadores. Com base nas escolhas dos jogadores, voc√™ prosseguir√° com a hist√≥ria, ajustando a narrativa conforme necess√°rio.\\n\\nRolagem de Dados: Em determinadas situa√ß√µes, como combates, testes de habilidade ou eventos aleat√≥rios, voc√™ deve solicitar que os jogadores rolem dados (por exemplo, um dado de 20 lados - d20). A partir do resultado fornecido pelos jogadores, voc√™ determinar√° o sucesso ou falha da a√ß√£o e narrar√° o desfecho.\\n\\nIniciar Combates: Quando um combate for iniciado, descreva o cen√°rio de batalha e solicite que os jogadores rolem dados para determinar a iniciativa. Durante o combate, voc√™ dever√° gerir as a√ß√µes dos NPCs inimigos e solicitar aos jogadores que rolem dados para atacar, defender e usar habilidades especiais. Com base nos resultados, voc√™ atualizar√° o estado do combate at√© que ele seja conclu√≠do.\\n\\nResultados e Consequ√™ncias: Sempre que uma a√ß√£o for resolvida, narre as consequ√™ncias de acordo com as regras do jogo e o resultado dos dados. Se um personagem for bem-sucedido, descreva o sucesso e suas repercuss√µes. Se falhar, narre as dificuldades ou complica√ß√µes que surgem.\\nLembre-se de manter a narrativa envolvente, adaptando-se √†s a√ß√µes dos jogadores e criando uma experi√™ncia din√¢mica e divertida. Boa aventura!\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Treine o modelo\n",
        "Agora vamos usar o `SFTTrainer` do Huggingface TRL! Mais documentos aqui: [documenta√ß√£o do TRL SFT](https://huggingface.co/docs/trl/sft_trainer). Fazemos 60 etapas para acelerar as coisas, mas voc√™ pode definir `num_train_epochs=1` para uma execu√ß√£o completa e desativar `max_steps=None`. Tamb√©m oferecemos suporte ao `DPOTrainer` do TRL!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        #max_steps = None,\n",
        "        num_train_epochs = 1, # For longer training runs!\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Infer√™ncia\n",
        "Vamos executar o modelo! O Unsloth tamb√©m torna a infer√™ncia nativamente 2x mais r√°pida! Voc√™ deve usar prompts semelhantes aos que voc√™ ajustou, caso contr√°rio, poder√° obter resultados ruins!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR3gIAX-SM2q"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "messages = [                    # Change below!\n",
        "    {\"role\": \"user\", \"content\": \"Continue the fibonacci sequence! Your input is 1, 1, 2, 3, 5, 8,\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrSvZObor0lY"
      },
      "source": [
        "Como criamos um chatbot real, voc√™ tamb√©m pode ter conversas mais longas adicionando manualmente conversas alternadas entre o usu√°rio e o assistente!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcbFUWEyQVaE"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "messages = [                         # Change below!\n",
        "    {\"role\": \"user\",      \"content\": \"Continue the fibonacci sequence! Your input is 1, 1, 2, 3, 5, 8\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"The fibonacci sequence continues as 13, 21, 34, 55 and 89.\"},\n",
        "    {\"role\": \"user\",      \"content\": \"What is France's tallest tower called?\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Salvando, carregando modelos finetuned\n",
        "Para salvar o modelo final como adaptadores LoRA, use `push_to_hub` do Huggingface para um salvamento online ou `save_pretrained` para um salvamento local.\n",
        "\n",
        "**[NOTA]** Isso salva SOMENTE os adaptadores LoRA, e n√£o o modelo completo. Para salvar em 16 bits ou GGUF, role para baixo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"RpgAiMasterLora\") # Local saving\n",
        "tokenizer.save_pretrained(\"RpgAiMasterLora\")\n",
        "model.push_to_hub(\"PauloFH/RpgAiMasterLora\", token = \os.getenv("HUGGIN_FACE_KEY")\") # Online saving\n",
        "tokenizer.push_to_hub(\"PauloFH/RpgAiMasterLora\", token = \os.getenv("HUGGIN_FACE_KEY")\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Agora, se voc√™ quiser carregar os adaptadores LoRA que acabamos de salvar para infer√™ncia, defina `False` como `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [],
      "source": [
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"RpgAiMasterLora\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "pass\n",
        "\n",
        "messages = [                    # Change below!\n",
        "    {\"role\": \"user\", \"content\": \"Describe anything special about a sequence. Your input is 1, 1, 2, 3, 5, 8,\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQMjaNrjsU5_"
      },
      "source": [
        "Voc√™ tamb√©m pode usar o `AutoModelForPeftCausalLM` do Hugging Face. Use-o somente se voc√™ n√£o tiver o `unsloth` instalado. Ele pode ser irremediavelmente lento, j√° que o download do modelo `4bit` n√£o √© suportado, e a **infer√™ncia do Unsloth √© 2x mais r√°pida**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFfaXG0WsQuE"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    # I highly do NOT suggest - use Unsloth if possible\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "    from transformers import AutoTokenizer\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        \"RpgAiMasterLora\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"RpgAiMasterLora\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOFzC441vCtq"
      },
      "source": [
        "<a name=\"Ollama\"></a>\n",
        "### Suporte Ollama\n",
        "\n",
        "[Unsloth](https://github.com/unslothai/unsloth) agora permite que voc√™ ajuste automaticamente e crie um [Modelfile](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) e exporte para [Ollama](https://ollama.com/)! Isso torna o ajuste fino muito mais f√°cil e fornece um fluxo de trabalho perfeito de `Unsloth` para `Ollama`!\n",
        "\n",
        "Vamos primeiro instalar `Ollama`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUxcyP_UfeLl"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "Em seguida, salvaremos o modelo em GGUF / llama.cpp\n",
        "\n",
        "Clonamos `llama.cpp` e salvamos por padr√£o em `q8_0`. Permitimos todos os m√©todos como `q4_k_m`. Use `save_pretrained_gguf` para salvar localmente e `push_to_hub_gguf` para fazer upload para HF.\n",
        "\n",
        "Alguns m√©todos quantitativos suportados (lista completa em nossa [p√°gina Wiki](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Convers√£o r√°pida. Alto uso de recursos, mas geralmente aceit√°vel.\n",
        "* `q4_k_m` - Recomendado. Usa Q6_K para metade dos tensores attention.wv e feed_forward.w2, sen√£o Q4_K.\n",
        "* `q5_k_m` - Recomendado. Usa Q6_K para metade dos tensores attention.wv e feed_forward.w2, sen√£o Q5_K.\n",
        "\n",
        "Tamb√©m oferecemos suporte para salvar em v√°rias op√ß√µes GGUF em uma lista! Isso pode acelerar as coisas em 10 minutos ou mais se voc√™ quiser v√°rios formatos de exporta√ß√£o!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if True: model.save_pretrained_gguf(\"RpgAiMaster\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False: model.push_to_hub_gguf(\"PauloFH/RpgAiMaster\", tokenizer, token = \os.getenv("HUGGIN_FACE_KEY")\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"RpgAiMaster\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"PauloFH/RpgAiMaster\", tokenizer, quantization_method = \"f16\", token = \os.getenv("HUGGIN_FACE_KEY")\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"RpgAiMaster\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"PauloFH/RpgAiMaster\", tokenizer, quantization_method = \"q4_k_m\", token = \os.getenv("HUGGIN_FACE_KEY")\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if True:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"PauloFH/RpgAiMaster\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \os.getenv("HUGGIN_FACE_KEY")\", # Get a token at https://huggingface.co/settings/tokens\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7lk6l0CuPXS"
      },
      "source": [
        "Usamos `subprocess` para iniciar `Ollama` de forma n√£o bloqueante! Em seu pr√≥prio desktop, voc√™ pode simplesmente abrir um novo `terminal` e digitar `ollama serve`, mas no Colab, temos que usar este hack!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcP9omF_tN7Q"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Wait for a few seconds for Ollama to load!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md3PExRLRhOc"
      },
      "source": [
        "`Ollama` precisa de um `Modelfile`, que especifica o formato de prompt do modelo. Vamos imprimir o gerado automaticamente pelo Unsloth:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h82vfNigRhiz"
      },
      "outputs": [],
      "source": [
        "print(tokenizer._ollama_modelfile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6cipBJBudxv"
      },
      "source": [
        "Agora criaremos um modelo `Ollama` chamado `unsloth_model` usando o `Modelfile` que geramos automaticamente!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDTUJv_QiaVh"
      },
      "outputs": [],
      "source": [
        "!ollama create unsloth_model -f ./model/Modelfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KSoKTKQukba"
      },
      "source": [
        "E agora podemos fazer infer√™ncia sobre isso via `Ollama`!\n",
        "\n",
        "Voc√™ tamb√©m pode fazer upload para `Ollama` e experimentar o aplicativo `Ollama` Desktop indo para https://www.ollama.com/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkp0uMrNpYaW"
      },
      "outputs": [],
      "source": [
        "!curl http://localhost:11434/api/chat -d '{ \\\n",
        "    \"model\": \"unsloth_model\", \\\n",
        "    \"messages\": [ \\\n",
        "        { \"role\": \"user\", \"content\": \"Continue the Fibonacci sequence: 1, 1, 2, 3, 5, 8,\" } \\\n",
        "    ] \\\n",
        "    }'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGPT interactive mode\n",
        "\n",
        "### ‚≠ê To run the finetuned model like in a ChatGPT style interface, first click the **| >_ |** button.\n",
        "![](https://raw.githubusercontent.com/unslothai/unsloth/nightly/images/Where_Terminal.png)\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "### ‚≠ê Then, type `ollama run unsloth_model`\n",
        "\n",
        "![](https://raw.githubusercontent.com/unslothai/unsloth/nightly/images/Terminal_Type.png)\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "### ‚≠ê And you have a CHatGPT style assistant!\n",
        "\n",
        "### Type any question you like and press `ENTER`. If you want to exit, hit `CTRL + D`\n",
        "![](https://raw.githubusercontent.com/unslothai/unsloth/nightly/images/Assistant.png)"
      ],
      "metadata": {
        "id": "XnMbhp7KsKhr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt9CHJqO6p30"
      },
      "source": [
        "E terminamos! Se voc√™ tiver alguma d√∫vida sobre o Unsloth, temos um canal [Discord](https://discord.gg/u54VK8m8tk)! Se voc√™ encontrar algum bug ou quiser se manter atualizado com as √∫ltimas novidades do LLM, ou precisar de ajuda, participar de projetos etc., sinta-se √† vontade para participar do nosso Discord!\n",
        "\n",
        "Experimente nosso [caderno Ollama CSV](https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing) para enviar CSVs para ajustes finos!\n",
        "\n",
        "Alguns outros links:\n",
        "1. Zephyr DPO 2x mais r√°pido [Colab gr√°tis](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)\n",
        "2. Llama 7b 2x mais r√°pido [Colab gr√°tis](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)\n",
        "3. TinyLlama 4x mais r√°pido Alpaca 52K completo em 1 hora [Colab gr√°tis](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)\n",
        "4. CodeLlama 34b 2x mais r√°pido [A100 em Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)\n",
        "5. Mistral 7b [vers√£o gratuita do Kaggle](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\n",
        "6. Tamb√©m fizemos um [blog](https://huggingface.co/blog/unsloth-trl) com ü§ó HuggingFace, e estamos nos [docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) do TRL!\n",
        "7. `ChatML` para conjuntos de dados ShareGPT, [caderno de conversa√ß√£o](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing)\n",
        "8. Complementa√ß√µes de texto como escrita de romance [caderno](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)\n",
        "9. [**NOVO**] Tornamos o Phi-3 Medium / Mini **2x mais r√°pido**! Veja nosso [caderno Phi-3 Medium](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)\n",
        "\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://ollama.com/\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/nightly/images/ollama.png\" height=\"44\"></a>\n",
        "<a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Entre no Discord se precisar de ajuda + ‚≠ê <i>Adicione uma estrela para n√≥s no <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f505fcc71858474e8474201e3aae5d37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22e1dd61829149eb9014e60a3fbb315e",
              "IPY_MODEL_05005f157d65410ea831c6e7e48ecf9c",
              "IPY_MODEL_240fb241a53c4e96b54b655b0e5ec7c6"
            ],
            "layout": "IPY_MODEL_2c4b5d605822410d9c79d716106acd39"
          }
        },
        "22e1dd61829149eb9014e60a3fbb315e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35cc6f434cb74f988fdd3d16a89ca46e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_57c0f79c22a044d59a861f305c6e2c68",
            "value": "model.safetensors:‚Äá‚Äá65%"
          }
        },
        "05005f157d65410ea831c6e7e48ecf9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e68b1673cfea443b8fd090f2cd96606b",
            "max": 5702746390,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3d8d5c37e834fe0a6717db68bbf9791",
            "value": 3690987168
          }
        },
        "240fb241a53c4e96b54b655b0e5ec7c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65ca2f08912b4111aa8ec4639e6e0fb4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_db37443e50554e23b911f1626e0c2381",
            "value": "‚Äá3.69G/5.70G‚Äá[00:43&lt;00:13,‚Äá150MB/s]"
          }
        },
        "2c4b5d605822410d9c79d716106acd39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35cc6f434cb74f988fdd3d16a89ca46e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57c0f79c22a044d59a861f305c6e2c68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e68b1673cfea443b8fd090f2cd96606b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3d8d5c37e834fe0a6717db68bbf9791": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "65ca2f08912b4111aa8ec4639e6e0fb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db37443e50554e23b911f1626e0c2381": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}