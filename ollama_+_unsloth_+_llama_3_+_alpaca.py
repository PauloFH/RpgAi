# -*- coding: utf-8 -*-
"""C√≥pia de Ollama  + Unsloth + Llama-3 + Alpaca.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t6nrXyVZL-jlzVpsX4mV1LAJ0kdYkWmS

Para executar isso, pressione "*Runtime*" e pressione "*Run all*" em uma inst√¢ncia **gr√°tis** do Tesla T4 Google Colab!
<div class="align-center">
<a href="https://github.com/unslothai/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png" width="115"></a>
<a href="https://ollama.com/"><img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/ollama.png" height="44"></a>
<a href="https://discord.gg/u54VK8m8tk"><img src="https://github.com/unslothai/unsloth/raw/main/images/Discord button.png" width="145"></a>
<a href="https://ko-fi.com/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png" width="145"></a></a> Entre no Discord se precisar de ajuda + ‚≠ê <i>Marque-nos com uma estrela no <a href="https://github.com/unslothai/unsloth">Github</a> </i> ‚≠ê
</div>

Para instalar o Unsloth no seu pr√≥prio computador, siga as instru√ß√µes de instala√ß√£o na nossa p√°gina do Github [aqui](https://github.com/unslothai/unsloth#installation-instructions---conda).

Voc√™ aprender√° como fazer [prepara√ß√£o de dados](#Dados) e importar um CSV, como [treinar](#Treinar), como [executar o modelo](#Infer√™ncia) e [como exportar para o Ollama!](#Ollama)

[Unsloth](https://github.com/unslothai/unsloth) agora permite que voc√™ ajuste automaticamente e crie um [Modelfile](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) e exporte para [Ollama](https://ollama.com/)! Isso torna o ajuste fino muito mais f√°cil e fornece um fluxo de trabalho perfeito de `Unsloth` para `Ollama`!

**[NOVO]** Agora permitimos o upload de arquivos CSV e Excel - experimente [aqui](https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing) usando o conjunto de dados do Titanic.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install unsloth
# # Also get the latest nightly Unsloth!
# !pip uninstall unsloth -y && pip install --upgrade --no-cache-dir "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"

"""* Oferecemos suporte a Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen, TinyLlama, Vicuna, Open Hermes etc
* Oferecemos suporte a LoRA de 16 bits ou QLoRA de 4 bits. Ambos 2x mais r√°pidos.
* `max_seq_length` pode ser definido como qualquer coisa, j√° que fazemos o dimensionamento autom√°tico de RoPE por meio do m√©todo [kaiokendev's](https://kaiokendev.github.io/til).
* Com [PR 26037](https://github.com/huggingface/transformers/pull/26037), oferecemos suporte ao download de modelos de 4 bits **4x mais r√°pido**! [Nosso reposit√≥rio](https://huggingface.co/unsloth) tem modelos Llama e Mistral de 4 bits.
* [**NOVO**] Tornamos o Phi-3 M√©dio / Mini **2x mais r√°pido**! Veja nosso [caderno Phi-3 Medium](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)
"""

from unsloth import FastLanguageModel
import torch
max_seq_length = 8192 # Choose any! We auto support RoPE Scaling internally!
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/mistral-7b-v0.3-bnb-4bit",      # New Mistral v3 2x faster!
    "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    "unsloth/llama-3-8b-bnb-4bit",           # Llama-3 15 trillion tokens model 2x faster!
    "unsloth/llama-3-8b-Instruct-bnb-4bit",
    "unsloth/llama-3-70b-bnb-4bit",
    "unsloth/Phi-3-mini-4k-instruct",        # Phi-3 2x faster!
    "unsloth/Phi-3-medium-4k-instruct",
    "unsloth/mistral-7b-bnb-4bit",
    "unsloth/gemma-7b-bnb-4bit",             # Gemma 2.2x faster!
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Meta-Llama-3.1-8B-bnb-4bit",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)

"""Agora adicionamos adaptadores LoRA, ent√£o precisamos atualizar apenas 1 a 10% de todos os par√¢metros!

"""

model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = True,  # N√≥s apoiamos LoRA estabilizado por classifica√ß√£o
    loftq_config = None, # And LoftQ
)

"""<a name="Data"></a>
### Prepara√ß√£o de dados
Agora usamos o conjunto de dados Alpaca de [vicgalle](https://huggingface.co/datasets/vicgalle/alpaca-gpt4), que √© uma vers√£o de 52K do [conjunto de dados Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) original gerado do GPT4. Voc√™ pode substituir esta se√ß√£o de c√≥digo pela sua pr√≥pria prepara√ß√£o de dados.
"""

from datasets import load_dataset
dataset = load_dataset("vicgalle/alpaca-gpt4", split = "train")
print(dataset.column_names)

"""Um problema √© que esse conjunto de dados tem v√°rias colunas. Para que `Ollama` e `llama.cpp` funcionem como um Chatbot `ChatGPT` personalizado, precisamos ter apenas 2 colunas - uma coluna `instruction` e uma coluna `output`."""

print(dataset.column_names)

"""Para resolver isso, faremos o seguinte:
* Mesclar todas as colunas em 1 prompt de instru√ß√£o.
* Lembre-se de que os LLMs s√£o preditores de texto, ent√£o podemos personalizar a instru√ß√£o para o que quisermos!
* Use a fun√ß√£o `to_sharegpt` para fazer esse processo de mesclagem de colunas!

Por exemplo, abaixo, em nosso [notebook de ajuste fino do Titanic CSV](https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing), mesclamos v√°rias colunas em 1 prompt:

<img src="https://raw.githubusercontent.com/unslothai/unsloth/nightly/images/Merge.png" height="100">

Para mesclar v√°rias colunas em 1, use `merged_prompt`.
* Coloque todas as colunas entre chaves `{}`.
* O texto opcional deve ser inclu√≠do em `[[]]`. Por exemplo, se a coluna "Pclass" estiver vazia, a fun√ß√£o de mesclagem n√£o mostrar√° o texto e ignorar√° isso. Isso √© √∫til para conjuntos de dados com valores ausentes.
* Voc√™ pode selecionar todas as colunas ou algumas!
* Selecione a sa√≠da ou a coluna de destino/previs√£o em `output_column_name`. Para o conjunto de dados Alpaca, ser√° `output`.

Para fazer o ajuste fino lidar com v√°rias voltas (como no ChatGPT), temos que criar um conjunto de dados "falso" com v√°rias voltas - usamos `conversation_extension` para selecionar aleatoriamente algumas conversas do conjunto de dados e agrup√°-las em 1 conversa.
"""

from datasets import load_dataset, Dataset
import json
from unsloth import to_sharegpt

# Carregar o dataset "vicgalle/alpaca-gpt4" e descartar o campo "text"
dataset1 = load_dataset("vicgalle/alpaca-gpt4", split="train").remove_columns(["text"])

# Carregar o JSON com os campos ['instruction', 'input', 'output']
with open("lostmine_dataset.json") as f:
    data_json = json.load(f)

data_dict = {
    "instruction": [example["instruction"] for example in data_json],
    "input": [example["input"] for example in data_json],
    "output": [example["output"] for example in data_json]
}
# Converter o JSON para um Dataset Hugging Face
dataset2 = Dataset.from_dict(data_dict)
print(dataset1.column_names)
print(dataset2.column_names)
# Concatenar os datasets
final_dataset = Dataset.from_dict({
    "instruction": dataset1["instruction"] + dataset2["instruction"],
    "input": dataset1["input"] + dataset2["input"],
    "output": dataset1["output"] + dataset2["output"]
})
# Concatenar os campos 'instruction' e 'input' no dataset1
dataset = to_sharegpt(
    final_dataset,
    merged_prompt="{instruction}[[\nYour input is:\n{input}]]",
    output_column_name="output",
    conversation_extension=5,
)
# Verificar os campos finais
print(dataset.column_names)

"""Por fim, use `standardize_sharegpt` para corrigir o conjunto de dados!"""

from unsloth import standardize_sharegpt
dataset = standardize_sharegpt(dataset)

"""### Modelos de bate-papo personaliz√°veis

Voc√™ tamb√©m precisa especificar um modelo de bate-papo. Anteriormente, voc√™ podia usar o formato Alpaca, conforme mostrado abaixo.
"""

alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

"""Agora, voc√™ tem que usar `{INPUT}` para a instru√ß√£o e `{OUTPUT}` para a resposta.

Tamb√©m permitimos que voc√™ use um campo opcional `{SYSTEM}`. Isso √© √∫til para o Ollama quando voc√™ quer usar um prompt de sistema personalizado (tamb√©m como no ChatGPT).

Voc√™ tamb√©m n√£o pode colocar um campo `{SYSTEM}` e apenas colocar texto simples.

```python
chat_template = '''{SYSTEM}
USU√ÅRIO: {INPUT}
ASSISTENTE: {OUTPUT}'''
```

Use abaixo se quiser usar o formato de prompt Llama-3. Voc√™ deve usar o modelo `instruct` e n√£o o `base` se usar isso!
```python
chat_template = '''<|begin_of_text|><|start_header_id|>sistema<|end_header_id|>

{SISTEMA}<|eot_id|><|start_header_id|>usu√°rio<|end_header_id|>

{ENTRADA}<|eot_id|><|start_header_id|>assistente<|end_header_id|>

{SA√çDA}<|eot_id|>'''
```

Para o formato ChatML:
```python
chat_template = '''<|im_start|>sistema
{SISTEMA}<|im_end|>
<|im_start|>usu√°rio
{ENTRADA}<|im_end|>
<|im_start|>assistente
{SA√çDA}<|im_end|>'''
```

O problema √© que o formato Alpaca tem 3 campos, enquanto os chatbots estilo OpenAI devem usar apenas 2 campos (instru√ß√£o e resposta). √â por isso que usamos a fun√ß√£o `to_sharegpt` para mesclar essas colunas em 1.
"""

chat_template = """Below are some instructions that describe some tasks. Write responses that appropriately complete each request.

### Instruction:
{INPUT}

### Response:
{OUTPUT}"""

from unsloth import apply_chat_template
dataset = apply_chat_template(
    dataset,
    tokenizer = tokenizer,
    chat_template = chat_template,
    default_system_message = "Voc√™ √© uma Intelig√™ncia Artificial atuando como Mestre de RPG de mesa, semelhante ao Dungeons & Dragons. Sua tarefa √© narrar hist√≥rias, conduzir aventuras e interagir com os jogadores de acordo com as regras do jogo. Aqui est√£o suas diretrizes:\n\nReceber Fichas de Personagem: Quando um jogador fornecer a ficha do seu personagem, voc√™ deve analisar os atributos, habilidades, equipamentos e hist√≥rico do personagem.\n\nSelecionar e Narrar Hist√≥rias: Com base na hist√≥ria selecionada, voc√™ iniciar√° a narrativa, descrevendo o ambiente, NPCs (Personagens N√£o Jog√°veis) e eventos. Use descri√ß√µes v√≠vidas para imergir os jogadores na aventura.\n\nIntera√ß√£o e Decis√µes dos Jogadores: Durante a narrativa, voc√™ dever√° propor situa√ß√µes que exijam decis√µes dos jogadores. Com base nas escolhas dos jogadores, voc√™ prosseguir√° com a hist√≥ria, ajustando a narrativa conforme necess√°rio.\n\nRolagem de Dados: Em determinadas situa√ß√µes, como combates, testes de habilidade ou eventos aleat√≥rios, voc√™ deve solicitar que os jogadores rolem dados (por exemplo, um dado de 20 lados - d20). A partir do resultado fornecido pelos jogadores, voc√™ determinar√° o sucesso ou falha da a√ß√£o e narrar√° o desfecho.\n\nIniciar Combates: Quando um combate for iniciado, descreva o cen√°rio de batalha e solicite que os jogadores rolem dados para determinar a iniciativa. Durante o combate, voc√™ dever√° gerir as a√ß√µes dos NPCs inimigos e solicitar aos jogadores que rolem dados para atacar, defender e usar habilidades especiais. Com base nos resultados, voc√™ atualizar√° o estado do combate at√© que ele seja conclu√≠do.\n\nResultados e Consequ√™ncias: Sempre que uma a√ß√£o for resolvida, narre as consequ√™ncias de acordo com as regras do jogo e o resultado dos dados. Se um personagem for bem-sucedido, descreva o sucesso e suas repercuss√µes. Se falhar, narre as dificuldades ou complica√ß√µes que surgem.\nLembre-se de manter a narrativa envolvente, adaptando-se √†s a√ß√µes dos jogadores e criando uma experi√™ncia din√¢mica e divertida. Boa aventura!"
)

"""<a name="Train"></a>
### Treine o modelo
Agora vamos usar o `SFTTrainer` do Huggingface TRL! Mais documentos aqui: [documenta√ß√£o do TRL SFT](https://huggingface.co/docs/trl/sft_trainer). Fazemos 60 etapas para acelerar as coisas, mas voc√™ pode definir `num_train_epochs=1` para uma execu√ß√£o completa e desativar `max_steps=None`. Tamb√©m oferecemos suporte ao `DPOTrainer` do TRL!
"""

from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported
trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,
    packing = False, # Can make training 5x faster for short sequences.
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        #max_steps = None,
        num_train_epochs = 1, # For longer training runs!
        learning_rate = 2e-4,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
    ),
)

#@title Show current memory stats
gpu_stats = torch.cuda.get_device_properties(0)
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
print(f"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.")
print(f"{start_gpu_memory} GB of memory reserved.")

trainer_stats = trainer.train()

#@title Show final memory and time stats
used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
used_memory_for_lora = round(used_memory - start_gpu_memory, 3)
used_percentage = round(used_memory         /max_memory*100, 3)
lora_percentage = round(used_memory_for_lora/max_memory*100, 3)
print(f"{trainer_stats.metrics['train_runtime']} seconds used for training.")
print(f"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.")
print(f"Peak reserved memory = {used_memory} GB.")
print(f"Peak reserved memory for training = {used_memory_for_lora} GB.")
print(f"Peak reserved memory % of max memory = {used_percentage} %.")
print(f"Peak reserved memory for training % of max memory = {lora_percentage} %.")

"""<a name="Inference"></a>
### Infer√™ncia
Vamos executar o modelo! O Unsloth tamb√©m torna a infer√™ncia nativamente 2x mais r√°pida! Voc√™ deve usar prompts semelhantes aos que voc√™ ajustou, caso contr√°rio, poder√° obter resultados ruins!
"""

FastLanguageModel.for_inference(model) # Enable native 2x faster inference
messages = [                    # Change below!
    {"role": "user", "content": "Continue the fibonacci sequence! Your input is 1, 1, 2, 3, 5, 8,"},
]
input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt = True,
    return_tensors = "pt",
).to("cuda")

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer, skip_prompt = True)
_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)

"""Como criamos um chatbot real, voc√™ tamb√©m pode ter conversas mais longas adicionando manualmente conversas alternadas entre o usu√°rio e o assistente!"""

FastLanguageModel.for_inference(model) # Enable native 2x faster inference
messages = [                         # Change below!
    {"role": "user",      "content": "Continue the fibonacci sequence! Your input is 1, 1, 2, 3, 5, 8"},
    {"role": "assistant", "content": "The fibonacci sequence continues as 13, 21, 34, 55 and 89."},
    {"role": "user",      "content": "What is France's tallest tower called?"},
]
input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt = True,
    return_tensors = "pt",
).to("cuda")

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer, skip_prompt = True)
_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)

"""<a name="Save"></a>
### Salvando, carregando modelos finetuned
Para salvar o modelo final como adaptadores LoRA, use `push_to_hub` do Huggingface para um salvamento online ou `save_pretrained` para um salvamento local.

**[NOTA]** Isso salva SOMENTE os adaptadores LoRA, e n√£o o modelo completo. Para salvar em 16 bits ou GGUF, role para baixo!
"""

model.save_pretrained("RpgAiMasterLora") # Local saving
tokenizer.save_pretrained("RpgAiMasterLora")
model.push_to_hub("PauloFH/RpgAiMasterLora", token = os.getenv("HUGGIN_FACE_KEY")) # Online saving
tokenizer.push_to_hub("PauloFH/RpgAiMasterLora", token = os.getenv("HUGGIN_FACE_KEY")) # Online saving

"""Agora, se voc√™ quiser carregar os adaptadores LoRA que acabamos de salvar para infer√™ncia, defina `False` como `True`:"""

if True:
    from unsloth import FastLanguageModel
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = "RpgAiMasterLora", # YOUR MODEL YOU USED FOR TRAINING
        max_seq_length = max_seq_length,
        dtype = dtype,
        load_in_4bit = load_in_4bit,
    )
    FastLanguageModel.for_inference(model) # Enable native 2x faster inference
pass

messages = [                    # Change below!
    {"role": "user", "content": "Describe anything special about a sequence. Your input is 1, 1, 2, 3, 5, 8,"},
]
input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt = True,
    return_tensors = "pt",
).to("cuda")

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer, skip_prompt = True)
_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)

"""Voc√™ tamb√©m pode usar o `AutoModelForPeftCausalLM` do Hugging Face. Use-o somente se voc√™ n√£o tiver o `unsloth` instalado. Ele pode ser irremediavelmente lento, j√° que o download do modelo `4bit` n√£o √© suportado, e a **infer√™ncia do Unsloth √© 2x mais r√°pida**."""

if False:
    # I highly do NOT suggest - use Unsloth if possible
    from peft import AutoPeftModelForCausalLM
    from transformers import AutoTokenizer
    model = AutoPeftModelForCausalLM.from_pretrained(
        "RpgAiMasterLora", # YOUR MODEL YOU USED FOR TRAINING
        load_in_4bit = load_in_4bit,
    )
    tokenizer = AutoTokenizer.from_pretrained("RpgAiMasterLora")

"""<a name="Ollama"></a>
### Suporte Ollama

[Unsloth](https://github.com/unslothai/unsloth) agora permite que voc√™ ajuste automaticamente e crie um [Modelfile](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) e exporte para [Ollama](https://ollama.com/)! Isso torna o ajuste fino muito mais f√°cil e fornece um fluxo de trabalho perfeito de `Unsloth` para `Ollama`!

Vamos primeiro instalar `Ollama`!
"""

!curl -fsSL https://ollama.com/install.sh | sh

"""Em seguida, salvaremos o modelo em GGUF / llama.cpp

Clonamos `llama.cpp` e salvamos por padr√£o em `q8_0`. Permitimos todos os m√©todos como `q4_k_m`. Use `save_pretrained_gguf` para salvar localmente e `push_to_hub_gguf` para fazer upload para HF.

Alguns m√©todos quantitativos suportados (lista completa em nossa [p√°gina Wiki](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):
* `q8_0` - Convers√£o r√°pida. Alto uso de recursos, mas geralmente aceit√°vel.
* `q4_k_m` - Recomendado. Usa Q6_K para metade dos tensores attention.wv e feed_forward.w2, sen√£o Q4_K.
* `q5_k_m` - Recomendado. Usa Q6_K para metade dos tensores attention.wv e feed_forward.w2, sen√£o Q5_K.

Tamb√©m oferecemos suporte para salvar em v√°rias op√ß√µes GGUF em uma lista! Isso pode acelerar as coisas em 10 minutos ou mais se voc√™ quiser v√°rios formatos de exporta√ß√£o!
"""

# Save to 8bit Q8_0
if True: model.save_pretrained_gguf("RpgAiMaster", tokenizer,)
# Remember to go to https://huggingface.co/settings/tokens for a token!
# And change hf to your username!
if False: model.push_to_hub_gguf("PauloFH/RpgAiMaster", tokenizer, token = os.getenv("HUGGIN_FACE_KEY"))

# Save to 16bit GGUF
if False: model.save_pretrained_gguf("RpgAiMaster", tokenizer, quantization_method = "f16")
if False: model.push_to_hub_gguf("PauloFH/RpgAiMaster", tokenizer, quantization_method = "f16", token = os.getenv("HUGGIN_FACE_KEY"))

# Save to q4_k_m GGUF
if False: model.save_pretrained_gguf("RpgAiMaster", tokenizer, quantization_method = "q4_k_m")
if False: model.push_to_hub_gguf("PauloFH/RpgAiMaster", tokenizer, quantization_method = "q4_k_m", token = os.getenv("HUGGIN_FACE_KEY"))

# Save to multiple GGUF options - much faster if you want multiple!
if True:
    model.push_to_hub_gguf(
        "PauloFH/RpgAiMaster", # Change hf to your username!
        tokenizer,
        quantization_method = ["q4_k_m", "q8_0", "q5_k_m",],
        token = os.getenv("HUGGIN_FACE_KEY"), # Get a token at https://huggingface.co/settings/tokens
    )

"""Usamos `subprocess` para iniciar `Ollama` de forma n√£o bloqueante! Em seu pr√≥prio desktop, voc√™ pode simplesmente abrir um novo `terminal` e digitar `ollama serve`, mas no Colab, temos que usar este hack!"""

import subprocess
subprocess.Popen(["ollama", "serve"])
import time
time.sleep(3) # Wait for a few seconds for Ollama to load!

"""`Ollama` precisa de um `Modelfile`, que especifica o formato de prompt do modelo. Vamos imprimir o gerado automaticamente pelo Unsloth:"""

print(tokenizer._ollama_modelfile)

"""Agora criaremos um modelo `Ollama` chamado `unsloth_model` usando o `Modelfile` que geramos automaticamente!"""

!ollama create unsloth_model -f ./model/Modelfile

"""E agora podemos fazer infer√™ncia sobre isso via `Ollama`!

Voc√™ tamb√©m pode fazer upload para `Ollama` e experimentar o aplicativo `Ollama` Desktop indo para https://www.ollama.com/
"""

!curl http://localhost:11434/api/chat -d '{ \
    "model": "unsloth_model", \
    "messages": [ \
        { "role": "user", "content": "Continue the Fibonacci sequence: 1, 1, 2, 3, 5, 8," } \
    ] \
    }'

"""# ChatGPT interactive mode

### ‚≠ê To run the finetuned model like in a ChatGPT style interface, first click the **| >_ |** button.
![](https://raw.githubusercontent.com/unslothai/unsloth/nightly/images/Where_Terminal.png)

---
---
---

### ‚≠ê Then, type `ollama run unsloth_model`

![](https://raw.githubusercontent.com/unslothai/unsloth/nightly/images/Terminal_Type.png)

---
---
---
### ‚≠ê And you have a CHatGPT style assistant!

### Type any question you like and press `ENTER`. If you want to exit, hit `CTRL + D`
![](https://raw.githubusercontent.com/unslothai/unsloth/nightly/images/Assistant.png)

E terminamos! Se voc√™ tiver alguma d√∫vida sobre o Unsloth, temos um canal [Discord](https://discord.gg/u54VK8m8tk)! Se voc√™ encontrar algum bug ou quiser se manter atualizado com as √∫ltimas novidades do LLM, ou precisar de ajuda, participar de projetos etc., sinta-se √† vontade para participar do nosso Discord!

Experimente nosso [caderno Ollama CSV](https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing) para enviar CSVs para ajustes finos!

Alguns outros links:
1. Zephyr DPO 2x mais r√°pido [Colab gr√°tis](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)
2. Llama 7b 2x mais r√°pido [Colab gr√°tis](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)
3. TinyLlama 4x mais r√°pido Alpaca 52K completo em 1 hora [Colab gr√°tis](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)
4. CodeLlama 34b 2x mais r√°pido [A100 em Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)
5. Mistral 7b [vers√£o gratuita do Kaggle](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)
6. Tamb√©m fizemos um [blog](https://huggingface.co/blog/unsloth-trl) com ü§ó HuggingFace, e estamos nos [docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) do TRL!
7. `ChatML` para conjuntos de dados ShareGPT, [caderno de conversa√ß√£o](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing)
8. Complementa√ß√µes de texto como escrita de romance [caderno](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)
9. [**NOVO**] Tornamos o Phi-3 Medium / Mini **2x mais r√°pido**! Veja nosso [caderno Phi-3 Medium](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)

<div class="align-center">
<a href="https://github.com/unslothai/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png" width="115"></a>
<a href="https://ollama.com/"><img src="https://raw.githubusercontent.com/unslothai/unsloth/nightly/images/ollama.png" height="44"></a>
<a href="https://discord.gg/u54VK8m8tk"><img src="https://github.com/unslothai/unsloth/raw/main/images/Discord button.png" width="145"></a>
<a href="https://ko-fi.com/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png" width="145"></a></a> Entre no Discord se precisar de ajuda + ‚≠ê <i>Adicione uma estrela para n√≥s no <a href="https://github.com/unslothai/unsloth">Github</a> </i> ‚≠ê
</div>
"""