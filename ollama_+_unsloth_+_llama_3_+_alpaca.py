# -*- coding: utf-8 -*-
"""Cópia de Ollama  + Unsloth + Llama-3 + Alpaca.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t6nrXyVZL-jlzVpsX4mV1LAJ0kdYkWmS

Para executar isso, pressione "*Runtime*" e pressione "*Run all*" em uma instância **grátis** do Tesla T4 Google Colab!
<div class="align-center">
<a href="https://github.com/unslothai/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png" width="115"></a>
<a href="https://ollama.com/"><img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/ollama.png" height="44"></a>
<a href="https://discord.gg/u54VK8m8tk"><img src="https://github.com/unslothai/unsloth/raw/main/images/Discord button.png" width="145"></a>
<a href="https://ko-fi.com/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png" width="145"></a></a> Entre no Discord se precisar de ajuda + ⭐ <i>Marque-nos com uma estrela no <a href="https://github.com/unslothai/unsloth">Github</a> </i> ⭐
</div>

Para instalar o Unsloth no seu próprio computador, siga as instruções de instalação na nossa página do Github [aqui](https://github.com/unslothai/unsloth#installation-instructions---conda).

Você aprenderá como fazer [preparação de dados](#Dados) e importar um CSV, como [treinar](#Treinar), como [executar o modelo](#Inferência) e [como exportar para o Ollama!](#Ollama)

[Unsloth](https://github.com/unslothai/unsloth) agora permite que você ajuste automaticamente e crie um [Modelfile](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) e exporte para [Ollama](https://ollama.com/)! Isso torna o ajuste fino muito mais fácil e fornece um fluxo de trabalho perfeito de `Unsloth` para `Ollama`!

**[NOVO]** Agora permitimos o upload de arquivos CSV e Excel - experimente [aqui](https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing) usando o conjunto de dados do Titanic.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install unsloth
# # Also get the latest nightly Unsloth!
# !pip uninstall unsloth -y && pip install --upgrade --no-cache-dir "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"

"""* Oferecemos suporte a Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen, TinyLlama, Vicuna, Open Hermes etc
* Oferecemos suporte a LoRA de 16 bits ou QLoRA de 4 bits. Ambos 2x mais rápidos.
* `max_seq_length` pode ser definido como qualquer coisa, já que fazemos o dimensionamento automático de RoPE por meio do método [kaiokendev's](https://kaiokendev.github.io/til).
* Com [PR 26037](https://github.com/huggingface/transformers/pull/26037), oferecemos suporte ao download de modelos de 4 bits **4x mais rápido**! [Nosso repositório](https://huggingface.co/unsloth) tem modelos Llama e Mistral de 4 bits.
* [**NOVO**] Tornamos o Phi-3 Médio / Mini **2x mais rápido**! Veja nosso [caderno Phi-3 Medium](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)
"""

from unsloth import FastLanguageModel
import torch
max_seq_length = 8192 # Choose any! We auto support RoPE Scaling internally!
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/mistral-7b-v0.3-bnb-4bit",      # New Mistral v3 2x faster!
    "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    "unsloth/llama-3-8b-bnb-4bit",           # Llama-3 15 trillion tokens model 2x faster!
    "unsloth/llama-3-8b-Instruct-bnb-4bit",
    "unsloth/llama-3-70b-bnb-4bit",
    "unsloth/Phi-3-mini-4k-instruct",        # Phi-3 2x faster!
    "unsloth/Phi-3-medium-4k-instruct",
    "unsloth/mistral-7b-bnb-4bit",
    "unsloth/gemma-7b-bnb-4bit",             # Gemma 2.2x faster!
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Meta-Llama-3.1-8B-bnb-4bit",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)

"""Agora adicionamos adaptadores LoRA, então precisamos atualizar apenas 1 a 10% de todos os parâmetros!

"""

model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = True,  # Nós apoiamos LoRA estabilizado por classificação
    loftq_config = None, # And LoftQ
)

"""<a name="Data"></a>
### Preparação de dados
Agora usamos o conjunto de dados Alpaca de [vicgalle](https://huggingface.co/datasets/vicgalle/alpaca-gpt4), que é uma versão de 52K do [conjunto de dados Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) original gerado do GPT4. Você pode substituir esta seção de código pela sua própria preparação de dados.
"""

from datasets import load_dataset
dataset = load_dataset("vicgalle/alpaca-gpt4", split = "train")
print(dataset.column_names)

"""Um problema é que esse conjunto de dados tem várias colunas. Para que `Ollama` e `llama.cpp` funcionem como um Chatbot `ChatGPT` personalizado, precisamos ter apenas 2 colunas - uma coluna `instruction` e uma coluna `output`."""

print(dataset.column_names)

"""Para resolver isso, faremos o seguinte:
* Mesclar todas as colunas em 1 prompt de instrução.
* Lembre-se de que os LLMs são preditores de texto, então podemos personalizar a instrução para o que quisermos!
* Use a função `to_sharegpt` para fazer esse processo de mesclagem de colunas!

Por exemplo, abaixo, em nosso [notebook de ajuste fino do Titanic CSV](https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing), mesclamos várias colunas em 1 prompt:

<img src="https://raw.githubusercontent.com/unslothai/unsloth/nightly/images/Merge.png" height="100">

Para mesclar várias colunas em 1, use `merged_prompt`.
* Coloque todas as colunas entre chaves `{}`.
* O texto opcional deve ser incluído em `[[]]`. Por exemplo, se a coluna "Pclass" estiver vazia, a função de mesclagem não mostrará o texto e ignorará isso. Isso é útil para conjuntos de dados com valores ausentes.
* Você pode selecionar todas as colunas ou algumas!
* Selecione a saída ou a coluna de destino/previsão em `output_column_name`. Para o conjunto de dados Alpaca, será `output`.

Para fazer o ajuste fino lidar com várias voltas (como no ChatGPT), temos que criar um conjunto de dados "falso" com várias voltas - usamos `conversation_extension` para selecionar aleatoriamente algumas conversas do conjunto de dados e agrupá-las em 1 conversa.
"""

from datasets import load_dataset, Dataset
import json
from unsloth import to_sharegpt

# Carregar o dataset "vicgalle/alpaca-gpt4" e descartar o campo "text"
dataset1 = load_dataset("vicgalle/alpaca-gpt4", split="train").remove_columns(["text"])

# Carregar o JSON com os campos ['instruction', 'input', 'output']
with open("lostmine_dataset.json") as f:
    data_json = json.load(f)

data_dict = {
    "instruction": [example["instruction"] for example in data_json],
    "input": [example["input"] for example in data_json],
    "output": [example["output"] for example in data_json]
}
# Converter o JSON para um Dataset Hugging Face
dataset2 = Dataset.from_dict(data_dict)
print(dataset1.column_names)
print(dataset2.column_names)
# Concatenar os datasets
final_dataset = Dataset.from_dict({
    "instruction": dataset1["instruction"] + dataset2["instruction"],
    "input": dataset1["input"] + dataset2["input"],
    "output": dataset1["output"] + dataset2["output"]
})
# Concatenar os campos 'instruction' e 'input' no dataset1
dataset = to_sharegpt(
    final_dataset,
    merged_prompt="{instruction}[[\nYour input is:\n{input}]]",
    output_column_name="output",
    conversation_extension=5,
)
# Verificar os campos finais
print(dataset.column_names)

"""Por fim, use `standardize_sharegpt` para corrigir o conjunto de dados!"""

from unsloth import standardize_sharegpt
dataset = standardize_sharegpt(dataset)

"""### Modelos de bate-papo personalizáveis

Você também precisa especificar um modelo de bate-papo. Anteriormente, você podia usar o formato Alpaca, conforme mostrado abaixo.
"""

alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

"""Agora, você tem que usar `{INPUT}` para a instrução e `{OUTPUT}` para a resposta.

Também permitimos que você use um campo opcional `{SYSTEM}`. Isso é útil para o Ollama quando você quer usar um prompt de sistema personalizado (também como no ChatGPT).

Você também não pode colocar um campo `{SYSTEM}` e apenas colocar texto simples.

```python
chat_template = '''{SYSTEM}
USUÁRIO: {INPUT}
ASSISTENTE: {OUTPUT}'''
```

Use abaixo se quiser usar o formato de prompt Llama-3. Você deve usar o modelo `instruct` e não o `base` se usar isso!
```python
chat_template = '''<|begin_of_text|><|start_header_id|>sistema<|end_header_id|>

{SISTEMA}<|eot_id|><|start_header_id|>usuário<|end_header_id|>

{ENTRADA}<|eot_id|><|start_header_id|>assistente<|end_header_id|>

{SAÍDA}<|eot_id|>'''
```

Para o formato ChatML:
```python
chat_template = '''<|im_start|>sistema
{SISTEMA}<|im_end|>
<|im_start|>usuário
{ENTRADA}<|im_end|>
<|im_start|>assistente
{SAÍDA}<|im_end|>'''
```

O problema é que o formato Alpaca tem 3 campos, enquanto os chatbots estilo OpenAI devem usar apenas 2 campos (instrução e resposta). É por isso que usamos a função `to_sharegpt` para mesclar essas colunas em 1.
"""

chat_template = """Below are some instructions that describe some tasks. Write responses that appropriately complete each request.

### Instruction:
{INPUT}

### Response:
{OUTPUT}"""

from unsloth import apply_chat_template
dataset = apply_chat_template(
    dataset,
    tokenizer = tokenizer,
    chat_template = chat_template,
    default_system_message = "Você é uma Inteligência Artificial atuando como Mestre de RPG de mesa, semelhante ao Dungeons & Dragons. Sua tarefa é narrar histórias, conduzir aventuras e interagir com os jogadores de acordo com as regras do jogo. Aqui estão suas diretrizes:\n\nReceber Fichas de Personagem: Quando um jogador fornecer a ficha do seu personagem, você deve analisar os atributos, habilidades, equipamentos e histórico do personagem.\n\nSelecionar e Narrar Histórias: Com base na história selecionada, você iniciará a narrativa, descrevendo o ambiente, NPCs (Personagens Não Jogáveis) e eventos. Use descrições vívidas para imergir os jogadores na aventura.\n\nInteração e Decisões dos Jogadores: Durante a narrativa, você deverá propor situações que exijam decisões dos jogadores. Com base nas escolhas dos jogadores, você prosseguirá com a história, ajustando a narrativa conforme necessário.\n\nRolagem de Dados: Em determinadas situações, como combates, testes de habilidade ou eventos aleatórios, você deve solicitar que os jogadores rolem dados (por exemplo, um dado de 20 lados - d20). A partir do resultado fornecido pelos jogadores, você determinará o sucesso ou falha da ação e narrará o desfecho.\n\nIniciar Combates: Quando um combate for iniciado, descreva o cenário de batalha e solicite que os jogadores rolem dados para determinar a iniciativa. Durante o combate, você deverá gerir as ações dos NPCs inimigos e solicitar aos jogadores que rolem dados para atacar, defender e usar habilidades especiais. Com base nos resultados, você atualizará o estado do combate até que ele seja concluído.\n\nResultados e Consequências: Sempre que uma ação for resolvida, narre as consequências de acordo com as regras do jogo e o resultado dos dados. Se um personagem for bem-sucedido, descreva o sucesso e suas repercussões. Se falhar, narre as dificuldades ou complicações que surgem.\nLembre-se de manter a narrativa envolvente, adaptando-se às ações dos jogadores e criando uma experiência dinâmica e divertida. Boa aventura!"
)

"""<a name="Train"></a>
### Treine o modelo
Agora vamos usar o `SFTTrainer` do Huggingface TRL! Mais documentos aqui: [documentação do TRL SFT](https://huggingface.co/docs/trl/sft_trainer). Fazemos 60 etapas para acelerar as coisas, mas você pode definir `num_train_epochs=1` para uma execução completa e desativar `max_steps=None`. Também oferecemos suporte ao `DPOTrainer` do TRL!
"""

from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported
trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,
    packing = False, # Can make training 5x faster for short sequences.
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        #max_steps = None,
        num_train_epochs = 1, # For longer training runs!
        learning_rate = 2e-4,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
    ),
)

#@title Show current memory stats
gpu_stats = torch.cuda.get_device_properties(0)
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
print(f"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.")
print(f"{start_gpu_memory} GB of memory reserved.")

trainer_stats = trainer.train()

#@title Show final memory and time stats
used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
used_memory_for_lora = round(used_memory - start_gpu_memory, 3)
used_percentage = round(used_memory         /max_memory*100, 3)
lora_percentage = round(used_memory_for_lora/max_memory*100, 3)
print(f"{trainer_stats.metrics['train_runtime']} seconds used for training.")
print(f"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.")
print(f"Peak reserved memory = {used_memory} GB.")
print(f"Peak reserved memory for training = {used_memory_for_lora} GB.")
print(f"Peak reserved memory % of max memory = {used_percentage} %.")
print(f"Peak reserved memory for training % of max memory = {lora_percentage} %.")

"""<a name="Inference"></a>
### Inferência
Vamos executar o modelo! O Unsloth também torna a inferência nativamente 2x mais rápida! Você deve usar prompts semelhantes aos que você ajustou, caso contrário, poderá obter resultados ruins!
"""

FastLanguageModel.for_inference(model) # Enable native 2x faster inference
messages = [                    # Change below!
    {"role": "user", "content": "Continue the fibonacci sequence! Your input is 1, 1, 2, 3, 5, 8,"},
]
input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt = True,
    return_tensors = "pt",
).to("cuda")

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer, skip_prompt = True)
_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)

"""Como criamos um chatbot real, você também pode ter conversas mais longas adicionando manualmente conversas alternadas entre o usuário e o assistente!"""

FastLanguageModel.for_inference(model) # Enable native 2x faster inference
messages = [                         # Change below!
    {"role": "user",      "content": "Continue the fibonacci sequence! Your input is 1, 1, 2, 3, 5, 8"},
    {"role": "assistant", "content": "The fibonacci sequence continues as 13, 21, 34, 55 and 89."},
    {"role": "user",      "content": "What is France's tallest tower called?"},
]
input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt = True,
    return_tensors = "pt",
).to("cuda")

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer, skip_prompt = True)
_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)

"""<a name="Save"></a>
### Salvando, carregando modelos finetuned
Para salvar o modelo final como adaptadores LoRA, use `push_to_hub` do Huggingface para um salvamento online ou `save_pretrained` para um salvamento local.

**[NOTA]** Isso salva SOMENTE os adaptadores LoRA, e não o modelo completo. Para salvar em 16 bits ou GGUF, role para baixo!
"""

model.save_pretrained("RpgAiMasterLora") # Local saving
tokenizer.save_pretrained("RpgAiMasterLora")
model.push_to_hub("PauloFH/RpgAiMasterLora", token = os.getenv("HUGGIN_FACE_KEY")) # Online saving
tokenizer.push_to_hub("PauloFH/RpgAiMasterLora", token = os.getenv("HUGGIN_FACE_KEY")) # Online saving

"""Agora, se você quiser carregar os adaptadores LoRA que acabamos de salvar para inferência, defina `False` como `True`:"""

if True:
    from unsloth import FastLanguageModel
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = "RpgAiMasterLora", # YOUR MODEL YOU USED FOR TRAINING
        max_seq_length = max_seq_length,
        dtype = dtype,
        load_in_4bit = load_in_4bit,
    )
    FastLanguageModel.for_inference(model) # Enable native 2x faster inference
pass

messages = [                    # Change below!
    {"role": "user", "content": "Describe anything special about a sequence. Your input is 1, 1, 2, 3, 5, 8,"},
]
input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt = True,
    return_tensors = "pt",
).to("cuda")

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer, skip_prompt = True)
_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)

"""Você também pode usar o `AutoModelForPeftCausalLM` do Hugging Face. Use-o somente se você não tiver o `unsloth` instalado. Ele pode ser irremediavelmente lento, já que o download do modelo `4bit` não é suportado, e a **inferência do Unsloth é 2x mais rápida**."""

if False:
    # I highly do NOT suggest - use Unsloth if possible
    from peft import AutoPeftModelForCausalLM
    from transformers import AutoTokenizer
    model = AutoPeftModelForCausalLM.from_pretrained(
        "RpgAiMasterLora", # YOUR MODEL YOU USED FOR TRAINING
        load_in_4bit = load_in_4bit,
    )
    tokenizer = AutoTokenizer.from_pretrained("RpgAiMasterLora")

"""<a name="Ollama"></a>
### Suporte Ollama

[Unsloth](https://github.com/unslothai/unsloth) agora permite que você ajuste automaticamente e crie um [Modelfile](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) e exporte para [Ollama](https://ollama.com/)! Isso torna o ajuste fino muito mais fácil e fornece um fluxo de trabalho perfeito de `Unsloth` para `Ollama`!

Vamos primeiro instalar `Ollama`!
"""

!curl -fsSL https://ollama.com/install.sh | sh

"""Em seguida, salvaremos o modelo em GGUF / llama.cpp

Clonamos `llama.cpp` e salvamos por padrão em `q8_0`. Permitimos todos os métodos como `q4_k_m`. Use `save_pretrained_gguf` para salvar localmente e `push_to_hub_gguf` para fazer upload para HF.

Alguns métodos quantitativos suportados (lista completa em nossa [página Wiki](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):
* `q8_0` - Conversão rápida. Alto uso de recursos, mas geralmente aceitável.
* `q4_k_m` - Recomendado. Usa Q6_K para metade dos tensores attention.wv e feed_forward.w2, senão Q4_K.
* `q5_k_m` - Recomendado. Usa Q6_K para metade dos tensores attention.wv e feed_forward.w2, senão Q5_K.

Também oferecemos suporte para salvar em várias opções GGUF em uma lista! Isso pode acelerar as coisas em 10 minutos ou mais se você quiser vários formatos de exportação!
"""

# Save to 8bit Q8_0
if True: model.save_pretrained_gguf("RpgAiMaster", tokenizer,)
# Remember to go to https://huggingface.co/settings/tokens for a token!
# And change hf to your username!
if False: model.push_to_hub_gguf("PauloFH/RpgAiMaster", tokenizer, token = os.getenv("HUGGIN_FACE_KEY"))

# Save to 16bit GGUF
if False: model.save_pretrained_gguf("RpgAiMaster", tokenizer, quantization_method = "f16")
if False: model.push_to_hub_gguf("PauloFH/RpgAiMaster", tokenizer, quantization_method = "f16", token = os.getenv("HUGGIN_FACE_KEY"))

# Save to q4_k_m GGUF
if False: model.save_pretrained_gguf("RpgAiMaster", tokenizer, quantization_method = "q4_k_m")
if False: model.push_to_hub_gguf("PauloFH/RpgAiMaster", tokenizer, quantization_method = "q4_k_m", token = os.getenv("HUGGIN_FACE_KEY"))

# Save to multiple GGUF options - much faster if you want multiple!
if True:
    model.push_to_hub_gguf(
        "PauloFH/RpgAiMaster", # Change hf to your username!
        tokenizer,
        quantization_method = ["q4_k_m", "q8_0", "q5_k_m",],
        token = os.getenv("HUGGIN_FACE_KEY"), # Get a token at https://huggingface.co/settings/tokens
    )

"""Usamos `subprocess` para iniciar `Ollama` de forma não bloqueante! Em seu próprio desktop, você pode simplesmente abrir um novo `terminal` e digitar `ollama serve`, mas no Colab, temos que usar este hack!"""

import subprocess
subprocess.Popen(["ollama", "serve"])
import time
time.sleep(3) # Wait for a few seconds for Ollama to load!

"""`Ollama` precisa de um `Modelfile`, que especifica o formato de prompt do modelo. Vamos imprimir o gerado automaticamente pelo Unsloth:"""

print(tokenizer._ollama_modelfile)

"""Agora criaremos um modelo `Ollama` chamado `unsloth_model` usando o `Modelfile` que geramos automaticamente!"""

!ollama create unsloth_model -f ./model/Modelfile

"""E agora podemos fazer inferência sobre isso via `Ollama`!

Você também pode fazer upload para `Ollama` e experimentar o aplicativo `Ollama` Desktop indo para https://www.ollama.com/
"""

!curl http://localhost:11434/api/chat -d '{ \
    "model": "unsloth_model", \
    "messages": [ \
        { "role": "user", "content": "Continue the Fibonacci sequence: 1, 1, 2, 3, 5, 8," } \
    ] \
    }'

"""# ChatGPT interactive mode

### ⭐ To run the finetuned model like in a ChatGPT style interface, first click the **| >_ |** button.
![](https://raw.githubusercontent.com/unslothai/unsloth/nightly/images/Where_Terminal.png)

---
---
---

### ⭐ Then, type `ollama run unsloth_model`

![](https://raw.githubusercontent.com/unslothai/unsloth/nightly/images/Terminal_Type.png)

---
---
---
### ⭐ And you have a CHatGPT style assistant!

### Type any question you like and press `ENTER`. If you want to exit, hit `CTRL + D`
![](https://raw.githubusercontent.com/unslothai/unsloth/nightly/images/Assistant.png)

E terminamos! Se você tiver alguma dúvida sobre o Unsloth, temos um canal [Discord](https://discord.gg/u54VK8m8tk)! Se você encontrar algum bug ou quiser se manter atualizado com as últimas novidades do LLM, ou precisar de ajuda, participar de projetos etc., sinta-se à vontade para participar do nosso Discord!

Experimente nosso [caderno Ollama CSV](https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing) para enviar CSVs para ajustes finos!

Alguns outros links:
1. Zephyr DPO 2x mais rápido [Colab grátis](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)
2. Llama 7b 2x mais rápido [Colab grátis](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)
3. TinyLlama 4x mais rápido Alpaca 52K completo em 1 hora [Colab grátis](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)
4. CodeLlama 34b 2x mais rápido [A100 em Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)
5. Mistral 7b [versão gratuita do Kaggle](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)
6. Também fizemos um [blog](https://huggingface.co/blog/unsloth-trl) com 🤗 HuggingFace, e estamos nos [docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) do TRL!
7. `ChatML` para conjuntos de dados ShareGPT, [caderno de conversação](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing)
8. Complementações de texto como escrita de romance [caderno](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)
9. [**NOVO**] Tornamos o Phi-3 Medium / Mini **2x mais rápido**! Veja nosso [caderno Phi-3 Medium](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)

<div class="align-center">
<a href="https://github.com/unslothai/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png" width="115"></a>
<a href="https://ollama.com/"><img src="https://raw.githubusercontent.com/unslothai/unsloth/nightly/images/ollama.png" height="44"></a>
<a href="https://discord.gg/u54VK8m8tk"><img src="https://github.com/unslothai/unsloth/raw/main/images/Discord button.png" width="145"></a>
<a href="https://ko-fi.com/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png" width="145"></a></a> Entre no Discord se precisar de ajuda + ⭐ <i>Adicione uma estrela para nós no <a href="https://github.com/unslothai/unsloth">Github</a> </i> ⭐
</div>
"""